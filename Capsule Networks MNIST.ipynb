{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from datetime import datetime\n",
    "from torch import nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define constants and dataset\n",
    "- TODO All these definitions should be done using CL args\n",
    "- Set important constant such as CUDA use and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False#torch.cuda.is_available()\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "dataset = torchvision.datasets.MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataSet and DataLoader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset('/datasets', train=True, download=False, transform=data_transforms['train'])\n",
    "test_data = dataset('/datasets', train=False, download=False, transform=data_transforms['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO List\n",
    "- TEST write routing algorithm\n",
    "- TEST write capsule layer architecture\n",
    "- TEST capsule network architecture\n",
    "- TEST Squash function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(tensor):\n",
    "    '''\n",
    "    TODO test\n",
    "    Squash function, defined in [1]. Works as a nonlinearity for CapsNets.\n",
    "    Input tensor will be of format (bs, units, C, H, W) or (bs, units, C)\n",
    "    Norm should be computed on the axis representing the number of units.\n",
    "    params:\n",
    "        tensor:    torch Variable containing n-dimensional tensor\n",
    "    output:\n",
    "        (||tensor||^2 / (1+ ||tensor||^2)) * tensor/||tensor||\n",
    "    '''\n",
    "    norm = torch.norm(tensor, p=2, dim=1, keepdim=True)\n",
    "    sq_norm = norm ** 2 # Avoid computing square twice\n",
    "        \n",
    "    return tensor.div(norm) * sq_norm/(1 + sq_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype of the capsule architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO add very long doc\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, input_channels, num_units, channels_per_unit, kernel_size, stride, routing, routing_iterations):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.input_channels = input_channels\n",
    "        self.num_units = num_units\n",
    "        self.channels_per_unit = channels_per_unit\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.routing = routing\n",
    "        self.routing_iterations = routing_iterations\n",
    "        \n",
    "        if self.routing:\n",
    "            \"\"\"\n",
    "            'W_ij is a weight matrix between each u_i, for i in (1, 32x6x6) in PrimaryCapsules and v_j, for j in (1, 10)'\n",
    "            Additionally, W_ij is an (8, 16) matrix.\n",
    "            This means the layer will have a parameter matrix of size (input_units * H_in * W_in, num_classes, input_channels, channels_per_unit).\n",
    "            To make it easier for us to define this matrix, let us assumme `input_units == original_input_units * H_in * W_in` when routing is active.\n",
    "            \"\"\"\n",
    "            self.weights = nn.Parameter(torch.randn(input_units, num_units, input_channels, channels_per_unit))     \n",
    "        else:\n",
    "            \"\"\"\n",
    "            For the PrimaryCaps layer (if the previous layer is not capsular too), the output should be the same as using multiple small \n",
    "            convolutional layers. Using a ModuleList facilitates interaction with all the units in a pythonic way.\n",
    "            Section 4,  3rd paragraph, describes the PrimaryCaps layer as having 32 units, each with 8 channels, with 9x9 kernel and stride 2.\n",
    "            \"\"\"\n",
    "            self.units = nn.ModuleList([nn.Conv2d(input_channels, channels_per_unit, kernel_size, stride) for unit in range(self.num_units)])\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Decide between applying routing or plain convolutions.\n",
    "        Routing is only used if between 2 consecutive layers\n",
    "        TODO try to implement routing as a method of the network and not the layers\n",
    "        \"\"\"\n",
    "        if self.routing:\n",
    "            return self._routing(x)\n",
    "        else:\n",
    "            return self._apply_conv_units(x)\n",
    "\n",
    "\n",
    "    def _routing(self, inputs):\n",
    "        \"\"\"\n",
    "        TODO add doc\n",
    "        This function is probably rather heavy. Should try profiling.\n",
    "        \"\"\"\n",
    "        batch_size = inputs.data.shape[0]\n",
    "        weights = torch.stack([self.weights] * batch_size, dim=0)\n",
    "        \n",
    "        current_votes = inputs.permute([0, 2, 1])\n",
    "        current_votes = torch.stack([current_votes] * self.num_units, dim=2)\n",
    "        current_votes = torch.stack([current_votes] * self.channels_per_unit, dim=-1)\n",
    "        \n",
    "        logits = Variable(torch.zeros(current_votes.data.shape))\n",
    "        pondered_votes = weights * current_votes  # Uji \n",
    "        \n",
    "        for iteration in range(self.routing_iterations):\n",
    "            couplings = F.softmax(logits, dim=-1)\n",
    "            out = couplings * pondered_votes\n",
    "            out = squash(out)\n",
    "            agreement = pondered_votes * out\n",
    "            logits = logits + agreement\n",
    "        \n",
    "        out = out.permute([0, 2, 1, 3, 4])\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def _apply_conv_units(self, x):\n",
    "        \"\"\"\n",
    "        Shape: (batch_size, input_channels, H, W) -> (batch_size, units, channels_per_unit, H', W')\n",
    "        H' and W' can be calculated using standard formulae for convolutional outputs\n",
    "        \"\"\"\n",
    "        caps_output = [unit(x) for unit in self.units]\n",
    "        caps_output = torch.stack(caps_output, dim=1)  # New dimension 1 will have size `units`\n",
    "        return caps_output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, conv_in_channels=1, conv_out_channels=256, conv_kernel_size=9, conv_stride=1, \n",
    "                 primary_units=32, primary_dim=8, primary_kernel_size=9, primary_stride=2,\n",
    "                 num_classes=10, digits_dim=16, dense_units_1=512, dense_units_2=1024, dense_units_3=784,\n",
    "                 routing_iterations=1):\n",
    "        \"\"\"\n",
    "        TODO Add very long doc for this...\n",
    "        dense_units_3 : int, number of pixels in an input image\n",
    "        \"\"\"\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels=conv_in_channels,\n",
    "                               out_channels=conv_out_channels,\n",
    "                               kernel_size=conv_kernel_size,\n",
    "                               stride=conv_stride)\n",
    "        self.primary_caps = CapsuleLayer(input_units=None, \n",
    "                                         input_channels=conv_out_channels,\n",
    "                                         num_units=primary_units,\n",
    "                                         channels_per_unit=primary_dim,\n",
    "                                         kernel_size=primary_kernel_size,\n",
    "                                         stride=primary_stride,\n",
    "                                         routing=False,\n",
    "                                         routing_iterations=routing_iterations)\n",
    "        self.digits_caps = CapsuleLayer(input_units=6*6*primary_units,\n",
    "                                        input_channels=primary_dim,\n",
    "                                        num_units=num_classes,\n",
    "                                        channels_per_unit=digits_dim,\n",
    "                                        kernel_size=0,\n",
    "                                        stride=0,\n",
    "                                        routing=True,\n",
    "                                        routing_iterations=routing_iterations)\n",
    "        self.decoder = nn.Sequential(nn.Linear(num_classes * digits_dim, dense_units_1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(dense_units_1, dense_units_2),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(dense_units_2, dense_units_3),\n",
    "                                     nn.Sigmoid())\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        TODO add doc\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        conv_out = self.conv0(x)\n",
    "        conv_out = F.relu(conv_out, inplace=False)\n",
    "        \n",
    "        primary_caps_out = self.primary_caps(conv_out)\n",
    "        squashed_primary_out = squash(primary_caps_out)\n",
    "        \n",
    "        digit_in = squashed_primary_out.view(batch_size, self.primary_caps.channels_per_unit, -1)  # -> (batch_size, primary_units, )\n",
    "        digit_out = self.digits_caps(digit_in)\n",
    "        \n",
    "        out = digit_out\n",
    "        while len(out.shape) > 2:\n",
    "            out = torch.norm(out, dim=-1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def margin_loss(votes, targets):\n",
    "    \"\"\"\n",
    "    TODO add doc\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsnet = CapsNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer definition according to default Tensorflow initiation\n",
    "From Tensorflow [AdamOptimizer docs](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer):\n",
    "```\n",
    "__init__(\n",
    "    learning_rate=0.001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    use_locking=False,\n",
    "    name='Adam'\n",
    ")```\n",
    "\n",
    "These are also the default values for torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(capsnet.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if use_gpu:\n",
    "    capsnet.cuda()\n",
    "    criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader_train, epochs, loss_fn, optimizer, data_loader_validation=None, patience=None):\n",
    "    model.train()\n",
    "    loss_history = torch.zeros(epochs)\n",
    "    acc_history = torch.zeros(epochs)\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss_sum = 0\n",
    "        for i, data in enumerate(data_loader_train):\n",
    "            print('starting batch #{:5.0f}'.format(i))\n",
    "            input, target = data\n",
    "            input_v, target_v = Variable(input), Variable(target)\n",
    "            if use_gpu:\n",
    "                input_v = input_v.cuda()\n",
    "                target_v = target_v.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(input_v)\n",
    "            \n",
    "            loss = loss_fn(log_probs, target_v)\n",
    "            loss_sum += loss.data[0]\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_history[epoch] = loss_sum / len(data_loader_train)\n",
    "        print('Loss in epoch {}: {}'.format(epoch+1, loss_history[epoch]))\n",
    "        torch.save(model, './caps_epoch{}.pth'.format(epoch))\n",
    "        \n",
    "    return loss_history, acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, num_samples):\n",
    "    hits = 0\n",
    "    model.eval()\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "        images, targets = data\n",
    "        images_v = Variable(images, volatile=True)\n",
    "        targets_v = Variable(targets, volatile=True)\n",
    "        if use_gpu:\n",
    "            images_v = images_v.cuda()\n",
    "            targets_v = targets_v.cuda()\n",
    "        \n",
    "        log_probs = model(images_v)\n",
    "        predictions = F.softmax(log_probs, dim=-1)\n",
    "        predictions = predictions.max(dim=-1)[1]\n",
    "        hits += (predictions == targets_v).sum().data[0]\n",
    "        \n",
    "    model.train()\n",
    "    return hits/num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting batch #    0\n",
      "starting batch #    1\n",
      "starting batch #    2\n",
      "starting batch #    3\n",
      "starting batch #    4\n",
      "starting batch #    5\n",
      "starting batch #    6\n",
      "starting batch #    7\n",
      "starting batch #    8\n",
      "starting batch #    9\n",
      "starting batch #   10\n",
      "starting batch #   11\n",
      "starting batch #   12\n",
      "starting batch #   13\n",
      "starting batch #   14\n",
      "starting batch #   15\n",
      "starting batch #   16\n",
      "starting batch #   17\n",
      "starting batch #   18\n",
      "starting batch #   19\n",
      "starting batch #   20\n",
      "starting batch #   21\n",
      "starting batch #   22\n",
      "starting batch #   23\n",
      "starting batch #   24\n",
      "starting batch #   25\n",
      "starting batch #   26\n",
      "starting batch #   27\n",
      "starting batch #   28\n",
      "starting batch #   29\n",
      "starting batch #   30\n",
      "starting batch #   31\n",
      "starting batch #   32\n",
      "starting batch #   33\n",
      "starting batch #   34\n",
      "starting batch #   35\n",
      "starting batch #   36\n",
      "starting batch #   37\n",
      "starting batch #   38\n",
      "starting batch #   39\n",
      "starting batch #   40\n",
      "starting batch #   41\n",
      "starting batch #   42\n",
      "starting batch #   43\n",
      "starting batch #   44\n",
      "starting batch #   45\n",
      "starting batch #   46\n",
      "starting batch #   47\n",
      "starting batch #   48\n",
      "starting batch #   49\n",
      "starting batch #   50\n",
      "starting batch #   51\n",
      "starting batch #   52\n",
      "starting batch #   53\n",
      "starting batch #   54\n",
      "starting batch #   55\n",
      "starting batch #   56\n",
      "starting batch #   57\n",
      "starting batch #   58\n",
      "starting batch #   59\n",
      "starting batch #   60\n",
      "starting batch #   61\n",
      "starting batch #   62\n",
      "starting batch #   63\n",
      "starting batch #   64\n",
      "starting batch #   65\n",
      "starting batch #   66\n",
      "starting batch #   67\n",
      "starting batch #   68\n",
      "starting batch #   69\n",
      "starting batch #   70\n",
      "starting batch #   71\n",
      "starting batch #   72\n",
      "starting batch #   73\n",
      "starting batch #   74\n",
      "starting batch #   75\n",
      "starting batch #   76\n",
      "starting batch #   77\n",
      "starting batch #   78\n",
      "starting batch #   79\n",
      "starting batch #   80\n",
      "starting batch #   81\n",
      "starting batch #   82\n",
      "starting batch #   83\n",
      "starting batch #   84\n",
      "starting batch #   85\n",
      "starting batch #   86\n",
      "starting batch #   87\n",
      "starting batch #   88\n",
      "starting batch #   89\n",
      "starting batch #   90\n",
      "starting batch #   91\n",
      "starting batch #   92\n",
      "starting batch #   93\n",
      "starting batch #   94\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1f7794946abe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcapsnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-64288ecf6061>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, data_loader_train, epochs, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mloss_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(capsnet, train_loader, epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "63\n",
      "93\n",
      "123\n",
      "154\n",
      "185\n",
      "217\n",
      "246\n",
      "276\n",
      "305\n",
      "332\n",
      "361\n",
      "391\n",
      "422\n",
      "449\n",
      "479\n",
      "510\n",
      "538\n",
      "567\n",
      "596\n",
      "624\n",
      "653\n",
      "682\n",
      "713\n",
      "745\n",
      "777\n",
      "806\n",
      "838\n",
      "867\n",
      "893\n",
      "924\n",
      "952\n",
      "983\n",
      "1013\n",
      "1039\n",
      "1068\n",
      "1096\n",
      "1125\n",
      "1152\n",
      "1179\n",
      "1207\n",
      "1235\n",
      "1266\n",
      "1294\n",
      "1322\n",
      "1348\n",
      "1378\n",
      "1406\n",
      "1435\n",
      "1464\n",
      "1495\n",
      "1525\n",
      "1554\n",
      "1577\n",
      "1607\n",
      "1636\n",
      "1666\n",
      "1697\n",
      "1728\n",
      "1758\n",
      "1788\n",
      "1816\n",
      "1847\n",
      "1872\n",
      "1900\n",
      "1929\n",
      "1956\n",
      "1988\n",
      "2014\n",
      "2043\n",
      "2074\n",
      "2103\n",
      "2135\n",
      "2166\n",
      "2192\n",
      "2223\n",
      "2253\n",
      "2284\n",
      "2315\n",
      "2344\n",
      "2373\n",
      "2401\n",
      "2428\n",
      "2458\n",
      "2489\n",
      "2520\n",
      "2549\n",
      "2579\n",
      "2610\n",
      "2641\n",
      "2671\n",
      "2700\n",
      "2730\n",
      "2759\n",
      "2791\n",
      "2820\n",
      "2850\n",
      "2878\n",
      "2906\n",
      "2936\n",
      "2967\n",
      "2997\n",
      "3026\n",
      "3056\n",
      "3085\n",
      "3114\n",
      "3144\n",
      "3172\n",
      "3201\n",
      "3231\n",
      "3262\n",
      "3290\n",
      "3320\n",
      "3351\n",
      "3381\n",
      "3412\n",
      "3441\n",
      "3469\n",
      "3495\n",
      "3520\n",
      "3547\n",
      "3576\n",
      "3606\n",
      "3634\n",
      "3665\n",
      "3694\n",
      "3725\n",
      "3753\n",
      "3784\n",
      "3812\n",
      "3841\n",
      "3869\n",
      "3896\n",
      "3925\n",
      "3950\n",
      "3982\n",
      "4010\n",
      "4041\n",
      "4070\n",
      "4098\n",
      "4124\n",
      "4154\n",
      "4184\n",
      "4215\n",
      "4245\n",
      "4276\n",
      "4307\n",
      "4337\n",
      "4367\n",
      "4398\n",
      "4424\n",
      "4454\n",
      "4481\n",
      "4512\n",
      "4539\n",
      "4568\n",
      "4599\n",
      "4631\n",
      "4659\n",
      "4691\n",
      "4722\n",
      "4752\n",
      "4784\n",
      "4815\n",
      "4844\n",
      "4874\n",
      "4906\n",
      "4938\n",
      "4969\n",
      "5001\n",
      "5033\n",
      "5065\n",
      "5097\n",
      "5129\n",
      "5160\n",
      "5189\n",
      "5218\n",
      "5250\n",
      "5281\n",
      "5312\n",
      "5344\n",
      "5376\n",
      "5407\n",
      "5435\n",
      "5462\n",
      "5491\n",
      "5518\n",
      "5547\n",
      "5575\n",
      "5604\n",
      "5631\n",
      "5663\n",
      "5691\n",
      "5723\n",
      "5755\n",
      "5787\n",
      "5819\n",
      "5851\n",
      "5883\n",
      "5914\n",
      "5946\n",
      "5978\n",
      "6009\n",
      "6040\n",
      "6070\n",
      "6099\n",
      "6128\n",
      "6158\n",
      "6189\n",
      "6221\n",
      "6249\n",
      "6280\n",
      "6310\n",
      "6341\n",
      "6371\n",
      "6400\n",
      "6431\n",
      "6462\n",
      "6494\n",
      "6526\n",
      "6556\n",
      "6586\n",
      "6613\n",
      "6645\n",
      "6677\n",
      "6707\n",
      "6735\n",
      "6767\n",
      "6799\n",
      "6830\n",
      "6862\n",
      "6894\n",
      "6924\n",
      "6956\n",
      "6986\n",
      "7017\n",
      "7049\n",
      "7080\n",
      "7112\n",
      "7144\n",
      "7176\n",
      "7208\n",
      "7239\n",
      "7270\n",
      "7301\n",
      "7328\n",
      "7358\n",
      "7387\n",
      "7419\n",
      "7451\n",
      "7483\n",
      "7515\n",
      "7544\n",
      "7575\n",
      "7607\n",
      "7638\n",
      "7669\n",
      "7699\n",
      "7730\n",
      "7762\n",
      "7793\n",
      "7825\n",
      "7855\n",
      "7887\n",
      "7919\n",
      "7951\n",
      "7980\n",
      "8012\n",
      "8044\n",
      "8076\n",
      "8108\n",
      "8140\n",
      "8172\n",
      "8204\n",
      "8236\n",
      "8267\n",
      "8299\n",
      "8331\n",
      "8362\n",
      "8394\n",
      "8426\n",
      "8455\n",
      "8484\n",
      "8515\n",
      "8547\n",
      "8578\n",
      "8610\n",
      "8641\n",
      "8673\n",
      "8705\n",
      "8736\n",
      "8768\n",
      "8800\n",
      "8832\n",
      "8864\n",
      "8894\n",
      "8926\n",
      "8956\n",
      "8986\n",
      "9017\n",
      "9049\n",
      "9077\n",
      "9108\n",
      "9137\n",
      "9164\n",
      "9192\n",
      "9221\n",
      "9249\n",
      "9277\n",
      "9307\n",
      "9335\n",
      "9364\n",
      "9379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9379"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(capsnet, test_loader, len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
