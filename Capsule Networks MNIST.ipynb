{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define constants and dataset\n",
    "- TODO All these definitions should be done using CL args\n",
    "- Set important constant such as CUDA use and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "batch_size = 32\n",
    "dataset = torchvision.datasets.MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataSet and DataLoader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset('/datasets', train=True, download=False)\n",
    "test_data = dataset('/datasets', train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO List\n",
    "- TODO write routing algorithm\n",
    "- TODO write capsule layer architecture\n",
    "- TODO write capsule network architecture\n",
    "- TEST Squash function\n",
    "- TODO Review Squash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _squash(tensor):\n",
    "    '''\n",
    "    TODO test\n",
    "    TODO review input format.\n",
    "    Squash function, defined in [1]. Works as a nonlinearity for CapsNets.\n",
    "    Input tensor will be of format (bs, units, C, H, W) or (bs, units, C)\n",
    "    Norm should be computed on the axis representing the number of units.\n",
    "    params:\n",
    "        tensor:    torch Variable containing n-dimensional tensor\n",
    "    output:\n",
    "        (||tensor||^2 / (1+ ||tensor||^2)) * tensor/||tensor||\n",
    "    '''\n",
    "    norm = torch.norm(tensor, p=2, dim=1, keepdim=True)\n",
    "    sq_norm = norm ** 2 # Avoid computing square twice\n",
    "        \n",
    "    return tensor.div(norm) * sq_norm/(1 + sq_norm)\n",
    "\n",
    "def _leaky_routing(logits):\n",
    "    '''\n",
    "    TODO write doc\n",
    "    \n",
    "    Parameters:\n",
    "        logits: Tensor of shape (bs, input_dim, output_dim)\n",
    "    \n",
    "    '''\n",
    "    leak = torch.zeros_like(logits)\n",
    "    leak = leak.sum(dim=2, keepdim=True)\n",
    "    leaky_logits = torch.cat([logits, leak], dim=2)\n",
    "    leaky_routing = F.softmax(leaky_logits, dim=2)\n",
    "    return leaky_routing[:, :, :-1, ...]\n",
    "\n",
    "def _update_routing(votes, biases, logit_shape, num_dims, input_dim, output_dim, num_routing, leaky):\n",
    "    '''\n",
    "    TODO test\n",
    "    TODO write doc\n",
    "    Parameters:\n",
    "        votes: Tensor (bs, input_dim, output_dim, output_atoms)\n",
    "        biases: Tensor (output_dim, output_atoms)\n",
    "        logit_shape: Tensor (bs, input_dim, output_dim)\n",
    "        num_dims:\n",
    "        input_dim: Integer.\n",
    "        output_dim: Integer.\n",
    "        num_routing: Integer. Number of routing iterations.\n",
    "        leaky: Boolean. Whether to use leaky routing or not.\n",
    "    '''\n",
    "\n",
    "    votes_t_shape = [3, 0, 1, 2] # [output_atoms, bs, input_dim, output_dim, ...]\n",
    "    for i in range(num_dims - 4):\n",
    "        votes_t_shape += [i + 4]\n",
    "    r_t_shape = [1, 2, 3, 0] # [bs, input_dim, output_dim, output_atoms, ...]\n",
    "    for i in range(num_dims - 4):\n",
    "        r_t_shape += [i + 4]\n",
    "    votes_trans = votes.permute(*votes_t_shape)\n",
    "    \n",
    "    activations = None\n",
    "    logits = Variable(torch.zeros(logit_shape))\n",
    "    for i in range(num_routing):\n",
    "        # route: [bs, input_dim, output_dim]\n",
    "        if leaky:\n",
    "            route = _leaky_routing(logits)\n",
    "        else:\n",
    "            route = F.softmax(logits, dim=2)\n",
    "        \n",
    "        preactivate_unrolled = route * votes_trans\n",
    "        preact_trans = preactivate_unrolled.permute(*r_t_shape)\n",
    "        preactivate = torch.sum(preact_trans, dim=1) + biases\n",
    "        activation = _squash(preactivate)\n",
    "        if activations is None:\n",
    "            activations = torch.unsqueeze(activation, dim=-1)\n",
    "        else:\n",
    "            activations = torch.cat([activations, torch.unsqueeze(activation, dim=-1)], dim=-1)            \n",
    "        #distances: [bs, input_dim, output_dim]\n",
    "        act_3d = torch.unsqueeze(activation, dim=1)\n",
    "        tile_shape = np.ones(num_dims, dtype=np.int32).tolist()\n",
    "        tile_shape[1] = input_dim\n",
    "        act_replicated = act_3d.repeat(*tile_shape)\n",
    "        distances = torch.sum(votes * act_replicated, dim=3)\n",
    "        logits += distances\n",
    "    return activations[..., -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,.,.) = \n",
       "  0.4259  0.4998\n",
       "  0.8518  0.8330\n",
       "[torch.FloatTensor of size 1x2x2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes = Variable(torch.Tensor(np.reshape(np.arange(8, dtype=np.float32), (1, 2, 2, 2))))\n",
    "biases = Variable(torch.zeros((2, 2)))\n",
    "logit_shape = (1, 2, 2)\n",
    "_update_routing(\n",
    "    votes,\n",
    "    biases, \n",
    "    logit_shape, \n",
    "    num_dims=4,\n",
    "    input_dim=2,\n",
    "    output_dim=2,\n",
    "    num_routing=1,\n",
    "    leaky=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us change the approach for now and try to define the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(nn.Module):\n",
    "    def __init__(self, input_units, input_channels, num_units, channels_per_unit, kernel_size, stride, routing):\n",
    "        super(CapsuleLayer, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.input_channels = input_channels\n",
    "        self.num_units = num_units\n",
    "        self.channels_per_unit = channels_per_unit\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.routing = routing\n",
    "        \n",
    "        if self.routing:\n",
    "            \"\"\"\n",
    "            'W_ij is a weight matrix between each u_i, for i in (1, 32x6x6) in PrimaryCapsules and v_j, for j in (1, 10)'\n",
    "            Additionally, W_ij is an (8, 16) matrix.\n",
    "            This means the layer will have a parameter matrix of size (input_units * H_in * W_in, num_classes, input_channels, channels_per_unit).\n",
    "            To make it easier for us to define this matrix, let us assumme `input_units == original_input_units * H_in * W_in` when routing is active.\n",
    "            \"\"\"\n",
    "            self.weights = nn.Parameter(torch.randn(input_units, num_units, input_channels, channels_per_unit))     \n",
    "        else:\n",
    "            \"\"\"\n",
    "            For the PrimaryCaps layer (if the previous layer is not capsular too), the output should be the same as using multiple small \n",
    "            convolutional layers. Using a ModuleList facilitates interaction with all the units in a pythonic way.\n",
    "            Section 4,  3rd paragraph, describes the PrimaryCaps layer as having 32 units, each with 8 channels, with 9x9 kernel and stride 2.\n",
    "            \"\"\"\n",
    "            self.units = nn.ModuleList([nn.Conv2d(input_channels, channels_per_unit, kernel_size, stride) for unit in range(self.num_units)])\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ### TODO implement routing between sucessive caps layers ###\n",
    "        if self.routing:\n",
    "            return self._routing(x)\n",
    "        else:\n",
    "            return self._apply_conv_units(x)\n",
    "    \n",
    "    \n",
    "    def _apply_conv_units(self, x):\n",
    "        # Shape: (batch_size, input_channels, H, W) -> (batch_size, units, channels_per_unit, H', W')\n",
    "        # H' and W' can be calculated using standard formulae for convolutional outputs\n",
    "        caps_output = [unit(x) for unit in self.units]\n",
    "        caps_output = torch.stack(caps_output, dim=1)  # New dimension 1 will have size `units`\n",
    "        return caps_output\n",
    "    \n",
    "    \n",
    "    def _routing(self, x):\n",
    "        #return torch.stack([self.weights]*self.num_units, dim=0)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-43-5394105ab616>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-43-5394105ab616>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    input_channels=,\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self, conv_in_channels=1, conv_out_channels=256, conv_kernel_size=9, conv_stride=1, \n",
    "                 primary_units=32, primary_dim=8, primary_kernel_size=9, primary_stride=2,\n",
    "                 num_classes=10, digits_dim=16, dense_units_1=512, dense_units_2=1024, dense_units_3=784):\n",
    "        \"\"\"\n",
    "        TODO Add very long doc for this...\n",
    "        dense_units_3 : int, number of pixels in an input image\n",
    "        \"\"\"\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels=conv_in_channels,\n",
    "                               out_channels=conv_out_channels,\n",
    "                               kernel_size=conv_kernel_size,\n",
    "                               stride=conv_stride)\n",
    "        self.primary_caps = CapsuleLayer(input_units=0, \n",
    "                                         input_channels=conv_out_channels,\n",
    "                                         num_units=primary_units,\n",
    "                                         channels_per_unit=primary_dim,\n",
    "                                         kernel_size=primary_kernel_size,\n",
    "                                         stride=primary_stride,\n",
    "                                         routing=False)\n",
    "        self.digits_caps = CapsuleLayer(input_units=primary_units,\n",
    "                                        input_channels=,\n",
    "                                        num_units=num_classes,\n",
    "                                        channels_per_unit=digits_dim,\n",
    "                                        routing=True)\n",
    "        self.decoder = nn.Sequential(nn.Linear(num_classes * digits_dim, dense_units_1),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(dense_units_1, dense_units_2),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(dense_units_2, dense_units_3),\n",
    "                                     nn.Sigmoid())\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        conv_out = self.conv0(x)\n",
    "        conv_out = F.relu(conv_out)\n",
    "        primary_caps_out = self.primary_caps(conv_out)\n",
    "        squashed_primary_out = _squash(primary_caps_out)\n",
    "        digit_in = squashed_primary_out.view(batch_size, self.primary_caps.num_units, -1)  # -> (batch_size, primary_units, )\n",
    "        \n",
    "        return squashed_primary_out # Change this as more layers are added to net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 8, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "cap0 = CapsuleLayer(input_units=0, input_channels=256, num_units=32, channels_per_unit=8, kernel_size=9, stride=2, routing=False)\n",
    "x = Variable(torch.randn(16, 256, 20, 20))\n",
    "out = cap0(x)\n",
    "print(out.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 10, 8, 34])\n"
     ]
    }
   ],
   "source": [
    "cap1 = CapsuleLayer(input_units=32, input_channels=8, num_units=10, channels_per_unit=16, kernel_size=9, stride=2, routing=True)\n",
    "out1 = cap1(out)\n",
    "print(out1.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 32, 8, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "capsnet = CapsNet()\n",
    "imgs = Variable(torch.randn(16, 1, 28, 28))\n",
    "out_net = capsnet(imgs)\n",
    "print(out_net.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer definition according to default Tensorflow initiation\n",
    "From Tensorflow [AdamOptimizer docs](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer):\n",
    "```\n",
    "__init__(\n",
    "    learning_rate=0.001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-08,\n",
    "    use_locking=False,\n",
    "    name='Adam'\n",
    ")```\n",
    "\n",
    "These are also the default values for torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
